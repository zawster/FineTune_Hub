{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebc1c13-99a7-448f-80ba-35e1f9bbcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29402c2d-5ab4-4604-9ba8-580fabf51738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 15:24:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:00:08.0 Off |                  Off |\n",
      "| 50%   65C    P8              42W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde1eb7b-4053-4166-86ff-0cb6ad61efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c9fa90-3fe2-4098-9f50-0e13a06630c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uqqq pip --progress-bar off\n",
    "# !pip install -Uqqq peft --progress-bar off\n",
    "# !pip install -Uqqq bitsandbytes --progress-bar off\n",
    "# !pip install -Uqqq trl --progress-bar off\n",
    "# !pip install git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e1beff-136e-459d-9712-669d80dd7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "# from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cae04c4-ac80-40cd-8e6e-2a95eddf6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1dad4f-45b9-4cdf-8693-11618697dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data-excel.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb13a151-73f2-4c4c-98f3-d1e9c93f99b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = ['headline', 'headline sentiment analysis', 'text', 'byline location','editorial notes','news value [nv] assessment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b52649-a59a-45ab-a33a-cd612cce98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[required_columns]\n",
    "df = df.fillna(\"\")\n",
    "df = df.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40ef095-3c4a-45e0-b94b-6a79cbe179c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b063795e-b635-4660-8779-5f684f3224b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d37ae25-f569-452a-aa7d-2da7a2ddc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_training_prompt(\n",
    "    text: str, sentiment: str, notes: str, rank:str, location:str\n",
    ") -> str:\n",
    "    return f\"\"\"{DEFAULT_SYSTEM_PROMPT}\n",
    "\n",
    "### Article:\n",
    "{text}\n",
    "\n",
    "### Sentiment:\n",
    "{sentiment}\n",
    "\n",
    "### editorial-notes:\n",
    "{notes}\n",
    "\n",
    "### Ranking:\n",
    "{rank}\n",
    "### Location:\n",
    "{location}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90569a84-83e2-4ca2-b2a2-b1c24a4393da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7affb6d1b0f04025ad8ab85479b70b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986c90a47b3a42d3bb7817d00819b62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mapper_function(example):\n",
    "    example['training_prompt'] = generate_training_prompt(\n",
    "        text=example['text'],\n",
    "        sentiment=example['headline sentiment analysis'],\n",
    "        notes=example['editorial notes'],\n",
    "        rank=example['news value [nv] assessment'],\n",
    "        location=example['byline location']\n",
    "    )\n",
    "    return example\n",
    "\n",
    "# Apply the mapper function to the dataset\n",
    "dataset = dataset_dict.map(mapper_function, remove_columns=required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885a14bd-7505-4fc0-8cb9-dc554a12237f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb39ca06c2314140bd069019b386c88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"Deci/DeciLM-7B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86bb6251-d509-44ac-b009-5f3c9a362692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k= prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b48cba3e-66f7-4fb0-b94d-46e74f100e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94250cb63f4646b8a1a57f50f5bfd8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace285aa27364bf3b8c78edce8238318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=1100,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Apply the tokenize function to the training prompts\n",
    "train_dataset = dataset['train'].map(lambda x: tokenize(x['training_prompt']))\n",
    "test_dataset = dataset['test'].map(lambda x: tokenize(x['training_prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84863c73-2773-4392-a5be-e3690b07b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_num_layers(model):\n",
    "    numbers = set()\n",
    "    for name, _ in model.named_parameters():\n",
    "        for number in re.findall(r'\\d+', name):\n",
    "            numbers.add(int(number))\n",
    "    return max(numbers)\n",
    "\n",
    "def get_last_layer_linears(model):\n",
    "    names = []\n",
    "    \n",
    "    num_layers = get_num_layers(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if str(num_layers) in name and not \"encoder\" in name:\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03fdbb84-2bb9-46b1-a44e-19c62402236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = LoraConfig(\n",
    "    r=8,  # Increased from 2 to 4\n",
    "    lora_alpha=64,  # Increased from 32 to 64\n",
    "    target_modules=get_last_layer_linears(model),  # Ensure this targets the right layers\n",
    "    lora_dropout=0.03,  # Reduced from 0.05 to 0.03 to allow for more utilization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model_p = get_peft_model(model_k, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c722feac-f441-40c1-bc49-cccc285766f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 2:18:41, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=290, training_loss=1713992047.2275863, metrics={'train_runtime': 8349.8772, 'train_samples_per_second': 1.145, 'train_steps_per_second': 0.035, 'total_flos': 4.23534188740608e+17, 'train_loss': 1713992047.2275863, 'epoch': 9.707112970711297})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    output_dir=\"finetune_DECILM\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model_p,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,    \n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model_p.config.use_cache = False\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153488f1-db2d-4e8d-9075-24660f999638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "#     model, type(model)\n",
    "# )\n",
    "# if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "#     print(\"compiling the model\")\n",
    "#     model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f8fff0d-1801-4761-b09a-188dc201b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc59addd-e0ab-414c-a127-1db037e41ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p.save_pretrained(\"trained-model\")\n",
    "\n",
    "PEFT_MODEL = \"trained-model\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     config.base_model_name_or_path,\n",
    "#     return_dict=True,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d116adb-1465-464c-b7ca-cda1ad003152",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 100\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6221bdbd-eee7-425f-9974-fe247804405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_prompt(\n",
    "    text: str) -> str:\n",
    "    return f\"\"\"{DEFAULT_SYSTEM_PROMPT}\n",
    "\n",
    "### Article:\n",
    "{text}\n",
    "\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fd91731-0c62-40a5-8bb6-1e0de4cd2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = generate_query_prompt(\"\"\"Facebook Inc. knows, in acute detail, that its platforms “We’re going to defend our record.”\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c15652ab-48aa-4654-80ff-f664a487bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Article:\n",
      "Facebook Inc. knows, in acute detail, that its platforms “We’re going to defend our record.”\n",
      "\n",
      "### Response:\n",
      "### Article:\n",
      "Facebook Inc. knows, in acute detail, that its platforms are being used to spread misinformation and hate speech. The company has been criticized for not doing enough to combat the problem.\n",
      "\n",
      "### Article:\n",
      "Facebook Inc. knows, in acute detail, that its platforms are being used to spread misinformation and hate speech. The company has been criticized for not doing enough to combat the problem.\n",
      "\n",
      "### Response\n",
      "CPU times: user 6.29 s, sys: 104 ms, total: 6.39 s\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = qry\n",
    "device = \"cuda\"\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True, skip_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186d6fee-3207-45ea-8acb-447292fe656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392aee7-5499-4c9d-8e33-3680a40592ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
